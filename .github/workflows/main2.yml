name: Weekly News Scraper & Claude Analysis

on:
  schedule:
    - cron: '0 6 * * 0'  # Sundays at 06:00 UTC
  workflow_dispatch:

jobs:
  scrape-analyze-and-email:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      # 1) Scrape all 16 static sites from news_scraper.py
      - name: Scrape static sites (news_scraper.py)
        run: |
          python news_scraper.py
          echo "---- scraped_data after static sites ----"
          ls -la scraped_data || true

      # 2) Scrape Radio-Canada sites
      - name: Scrape Radio-Canada (rc.py)
        run: |
          python rc.py
          echo "---- scraped_data after RC ----"
          ls -la scraped_data || true

      # 3) Merge the two JSON files into one for Claude
      - name: Merge static + RC JSON files
        shell: bash
        run: |
          set -e
          # Find the newest JSON file from each scraper
          STATIC_FILE=$(ls -t scraped_data/news_articles_*.json 2>/dev/null | head -n 1 || true)
          RC_FILE=$(ls -t scraped_data/radio_canada_articles_*.json 2>/dev/null | head -n 1 || true)

          # Check that we have at least the main static file
          if [ -z "$STATIC_FILE" ]; then
            echo "::error::No static news_articles_*.json file found. Stopping."
            exit 1
          fi
          
          # Define the final merged output file path
          MERGED_FILE="scraped_data/merged_articles_for_claude_$(date +%Y%m%d).json"

          if [ -z "$RC_FILE" ]; then
            echo "::warning::No radio_canada_articles_*.json file found. Proceeding with static file only."
            cp "$STATIC_FILE" "$MERGED_FILE"
          else
            echo "Merging $STATIC_FILE and $RC_FILE..."
            # Use Python to merge the two JSON arrays and deduplicate by URL
            python - <<'PY'
import json, os
def load(p):
    try:
        with open(p, "r", encoding="utf-8") as f: return json.load(f)
    except Exception as e:
        print(f"Error loading {p}: {e}")
        return []

static_articles = load(os.environ["STATIC_FILE"])
rc_articles = load(os.environ["RC_FILE"])
merged_file_path = os.environ["MERGED_FILE"]

seen_urls = set()
merged_articles = []

for article in static_articles + rc_articles:
    url = (article or {}).get("url")
    if url and url not in seen_urls:
        seen_urls.add(url)
        merged_articles.append(article)

with open(merged_file_path, "w", encoding="utf-8") as f:
    json.dump(merged_articles, f, ensure_ascii=False, indent=2)

print(f"Merged {len(static_articles)} static + {len(rc_articles)} RC articles -> {len(merged_articles)} unique articles into {merged_file_path}")
PY
          fi
          
          # Rename the merged file to what the analysis script expects
          # This is the file 'analyze_with_claude_single.py' will look for
          cp "$MERGED_FILE" "scraped_data/news_articles_$(date +%Y%m%d).json"

          echo "---- scraped_data after merge ----"
          ls -la scraped_data
        env:
          STATIC_FILE: ${{ steps.scrape-static.outputs.static_file }}
          RC_FILE: ${{ steps.scrape-rc.outputs.rc_file }}
          MERGED_FILE: scraped_data/merged_articles_for_claude_$(date +%Y%m%d).json

      # 4) Analyze merged file with Claude
      - name: Analyze merged news with Claude
        env:
          CLAUDE_API_KEY: ${{ secrets.CLAUDE_API_KEY }}
        run: |
          python analyze_with_claude_single.py
          echo "---- analysis folder contents ----"
          ls -la analysis || true

      # 5) Prepare analysis file for the email script
      - name: Prepare analysis file for email script
        shell: bash
        run: |
          set -e
          LATEST_ANALYSIS=$(ls -t analysis/claude_analysis_*.txt 2>/dev/null | head -n 1 || true)
          if [ -z "$LATEST_ANALYSIS" ]; then
            echo "::error::No analysis file found in analysis/ folder."
            exit 1
          fi
          
          # Copy the file to where send_analysis_email_Version2.py expects it
          DEST_FILE="scraped_data/$(date +%Y%m%d)_claude_analysis.txt"
          cp "$LATEST_ANALYSIS" "$DEST_FILE"
          echo "Copied $LATEST_ANALYSIS to $DEST_FILE for emailing."
          ls -la scraped_data

      # 6) Email latest analysis results
      - name: Email latest analysis results
        env:
          SMTP_SERVER: ${{ secrets.SMTP_SERVER }}
          SMTP_PORT: ${{ secrets.SMTP_PORT }}
          EMAIL_USER: ${{ secrets.EMAIL_USER }}
          EMAIL_PASSWORD: ${{ secrets.EMAIL_PASSWORD }}
          EMAIL_TO: ${{ secrets.EMAIL_TO }}
        run: |
          python send_analysis_email_Version2.py

      # 7) Upload artifacts for debugging
      - name: Upload results as workflow artifacts
        if: always() # Always run this, even if a step failed
        uses: actions/upload-artifact@v4
        with:
          name: news-analysis-results
          path: |
            scraped_data/
            analysis/
          if-no-files-found: warn
